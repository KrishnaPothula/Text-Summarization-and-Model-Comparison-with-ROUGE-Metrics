{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtTEXeEfCqZs55yCAVCc1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KrishnaPothula/Text-Summarization-and-Model-Comparison-with-ROUGE-Metrics/blob/main/Text_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate"
      ],
      "metadata": {
        "id": "qVpsEZIOwKn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]"
      ],
      "metadata": {
        "id": "p0ti2qCnwKkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "tdtiv-PhwKg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "pie4tVfJwKdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge"
      ],
      "metadata": {
        "id": "KLkc2umewKaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge-score"
      ],
      "metadata": {
        "id": "FhcHaOn3wKVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "id": "gF2pxtJQwR18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "id": "2RHGhkXJwVK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Data Loading"
      ],
      "metadata": {
        "id": "qsQ0meJi0VhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')"
      ],
      "metadata": {
        "id": "_VvXBZ9vwYep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataset['train'])  # You can use 'train', 'test', or 'validation'\n",
        "df1= pd.DataFrame(dataset['test'])\n",
        "df2= pd.DataFrame(dataset['validation'])"
      ],
      "metadata": {
        "id": "p4sAZBI6we_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = [df, df1, df2]\n",
        "\n",
        "for dataframe in dataframes:\n",
        "    dataframe.drop(columns=['id'], inplace=True)\n",
        "    dataframe.dropna(inplace=True)\n"
      ],
      "metadata": {
        "id": "CneT3JWvwjhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.shape"
      ],
      "metadata": {
        "id": "vxubM0tUwmVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "MXucjbjGwoNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset into dataframes (replace with your data loading code)\n",
        "df_train = pd.DataFrame(df)  # Training data\n",
        "df_val = pd.DataFrame(df1)  # Validation data\n",
        "df_test = pd.DataFrame(df2)  # Test data\n"
      ],
      "metadata": {
        "id": "D4ru5ZRLwwk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to create a dataset with a limited number of samples\n",
        "def prepare_dataset(dataframe, n_samples, dataset_name):\n",
        "    sample = dataframe.sample(n_samples, ignore_index=True)\n",
        "    sample.to_csv(f\"{dataset_name}.csv\", index=None)\n",
        "    raw_data = load_dataset(\"csv\", data_files=f\"{dataset_name}.csv\")\n",
        "    return raw_data"
      ],
      "metadata": {
        "id": "fLs_bA9twrgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sampling and Tokenization"
      ],
      "metadata": {
        "id": "FcJweM-s0f_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample a subset of your dataset for training and validation\n",
        "train_samples = 1000  # Adjust as needed\n",
        "val_samples = 500  # Adjust as needed\n",
        "test_samples = 50  # Adjust as needed\n",
        "\n",
        "raw_train = prepare_dataset(df_train, train_samples, 'train')\n",
        "raw_val = prepare_dataset(df_val, val_samples, 'val')\n",
        "\n",
        "# Define the tokenizer for each model\n",
        "model_checkpoints = {\n",
        "    \"BERTSUM\": \"bert-base-uncased\",\n",
        "    \"T5\": \"t5-small\",\n",
        "    \"GPT-2\": \"gpt2\",\n",
        "}\n",
        "\n",
        "tokenizers = {model: AutoTokenizer.from_pretrained(model_checkpoint) for model, model_checkpoint in model_checkpoints.items()}\n"
      ],
      "metadata": {
        "id": "iFLUrp7pzdDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning of the Models"
      ],
      "metadata": {
        "id": "KVUbF0DW0pGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for tokenizing\n",
        "def tokenize_data(batch, model):\n",
        "    input_texts = [f\"summarize: {doc}\" for doc in batch[\"article\"]] if model == \"T5\" else batch[\"article\"]\n",
        "    target_texts = batch[\"highlights\"] if model != \"BERTSUM\" else None\n",
        "\n",
        "    model_inputs = tokenizers[model](input_texts, max_length=512, truncation=True, padding=\"max_length\", return_tensors=\"pt\", text_target=target_texts)\n",
        "    return model_inputs\n",
        "\n",
        "# Fine-tuning the model for each model\n",
        "trained_models = {}\n",
        "\n",
        "for model, model_checkpoint in model_checkpoints.items():\n",
        "    print(f\"Fine-tuning {model} model...\")\n",
        "    # Tokenize the data\n",
        "    tokenized_train = raw_train.map(lambda batch: tokenize_data(batch, model), batched=True)\n",
        "    tokenized_val = raw_val.map(lambda batch: tokenize_data(batch, model), batched=True)\n",
        "\n",
        "    # Data Collator\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizers[model], model=model_checkpoint)\n",
        "\n",
        "    # Initialize the model\n",
        "    summarization_model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "    # Training Arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=f'{model.lower()}_summarization',\n",
        "        evaluation_strategy='steps',\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=2,  # Adjust batch size as needed\n",
        "        per_device_eval_batch_size=2,  # Adjust batch size as needed\n",
        "        weight_decay=0.01,\n",
        "        save_total_limit=3,\n",
        "        num_train_epochs=1,  # Reduced the number of epochs\n",
        "        eval_steps=100,  # Evaluate more frequently\n",
        "        save_steps=100,  # Save model more frequently\n",
        "        disable_tqdm=True,  # Disable tqdm progress bar for speed\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=summarization_model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train['train'],\n",
        "        eval_dataset=tokenized_val['train'],\n",
        "        tokenizer=tokenizers[model],\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=None,  # Compute ROUGE metrics after training\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Store the trained model\n",
        "    trained_models[model] = trainer.model\n"
      ],
      "metadata": {
        "id": "Wp6F4_MOzegO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation and ROUGE Metrics"
      ],
      "metadata": {
        "id": "R3KkSfpC0wII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation and ROUGE metrics for each model\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "for model, trained_model in trained_models.items():\n",
        "    # Preparing the test dataset\n",
        "    raw_test = prepare_dataset(df_test, test_samples, 'test')  # Adjust test_samples as needed\n",
        "\n",
        "    # Tokenize the test data\n",
        "    tokenized_test = raw_test.map(lambda batch: tokenize_data(batch, model), batched=True)\n",
        "\n",
        "    # Generate summaries for the test set\n",
        "    generated_summaries = []\n",
        "    for article in tokenized_test['train']['article']:\n",
        "        if model == \"GPT-2\":\n",
        "            # Use the pipeline for GPT-2\n",
        "            saved_model = pipeline('summarization', model=model_checkpoint)  # Load your trained model checkpoint\n",
        "            summary = saved_model(article, max_length=150, min_length=30, do_sample=False)\n",
        "            generated_summaries.append(summary[0]['summary_text'])\n",
        "        else:\n",
        "            # For BERTSUM and T5 models\n",
        "            with torch.no_grad():\n",
        "                summary_ids = trained_model.generate(article, max_length=150, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "                summary = tokenizers[model].decode(summary_ids[0], skip_special_tokens=True)\n",
        "            generated_summaries.append(summary)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "zOV3bReqzc9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute ROUGE metrics on the test set\n",
        "    def compute_rouge_metrics_test():\n",
        "        rouge_scores = {\n",
        "            'rouge1': [],\n",
        "            'rouge2': [],\n",
        "            'rougeL': []\n",
        "        }\n",
        "        for generated_summary, reference_summary in zip(generated_summaries, tokenized_test['train']['highlights']):\n",
        "            scores = scorer.score(reference_summary, generated_summary)\n",
        "            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
        "            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
        "            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
        "\n",
        "        return rouge_scores\n",
        "\n",
        "    # Compute ROUGE metrics on the test set\n",
        "    rouge_test_scores = compute_rouge_metrics_test()\n",
        "\n",
        "    # Create a table to compare the ROUGE scores\n",
        "    results_table = pd.DataFrame(rouge_test_scores, index=[model])\n",
        "\n",
        "    # Display the results table for the current model\n",
        "    print(f\"ROUGE Scores for {model} Model:\")\n",
        "    print(results_table)\n"
      ],
      "metadata": {
        "id": "x3FJRhTY2nO3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}